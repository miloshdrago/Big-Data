{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spurious Correlations in Highly Dimensional Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook aims at showing how PCA and Random Projection can solve the problem of spurious correlations in Big Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "from sklearn import random_projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>99990</th>\n",
       "      <th>99991</th>\n",
       "      <th>99992</th>\n",
       "      <th>99993</th>\n",
       "      <th>99994</th>\n",
       "      <th>99995</th>\n",
       "      <th>99996</th>\n",
       "      <th>99997</th>\n",
       "      <th>99998</th>\n",
       "      <th>99999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5777</th>\n",
       "      <td>0.013107</td>\n",
       "      <td>0.470582</td>\n",
       "      <td>0.782615</td>\n",
       "      <td>0.356561</td>\n",
       "      <td>0.713228</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.531202</td>\n",
       "      <td>0.149542</td>\n",
       "      <td>0.238533</td>\n",
       "      <td>0.156730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014578</td>\n",
       "      <td>0.703602</td>\n",
       "      <td>0.121921</td>\n",
       "      <td>0.809777</td>\n",
       "      <td>0.141529</td>\n",
       "      <td>0.051837</td>\n",
       "      <td>0.615442</td>\n",
       "      <td>0.598923</td>\n",
       "      <td>0.463951</td>\n",
       "      <td>0.278275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3783</th>\n",
       "      <td>0.960434</td>\n",
       "      <td>0.443701</td>\n",
       "      <td>0.571535</td>\n",
       "      <td>0.813105</td>\n",
       "      <td>0.448731</td>\n",
       "      <td>0.292695</td>\n",
       "      <td>0.378250</td>\n",
       "      <td>0.988330</td>\n",
       "      <td>0.737386</td>\n",
       "      <td>0.240126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865072</td>\n",
       "      <td>0.048426</td>\n",
       "      <td>0.556778</td>\n",
       "      <td>0.939422</td>\n",
       "      <td>0.430007</td>\n",
       "      <td>0.304750</td>\n",
       "      <td>0.629446</td>\n",
       "      <td>0.784474</td>\n",
       "      <td>0.027791</td>\n",
       "      <td>0.411822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>0.145563</td>\n",
       "      <td>0.462142</td>\n",
       "      <td>0.170783</td>\n",
       "      <td>0.475111</td>\n",
       "      <td>0.200657</td>\n",
       "      <td>0.647372</td>\n",
       "      <td>0.260204</td>\n",
       "      <td>0.386845</td>\n",
       "      <td>0.420123</td>\n",
       "      <td>0.275254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038677</td>\n",
       "      <td>0.744098</td>\n",
       "      <td>0.731141</td>\n",
       "      <td>0.329909</td>\n",
       "      <td>0.280043</td>\n",
       "      <td>0.441809</td>\n",
       "      <td>0.997663</td>\n",
       "      <td>0.531609</td>\n",
       "      <td>0.461333</td>\n",
       "      <td>0.930565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>0.415516</td>\n",
       "      <td>0.027677</td>\n",
       "      <td>0.297519</td>\n",
       "      <td>0.258756</td>\n",
       "      <td>0.415945</td>\n",
       "      <td>0.530397</td>\n",
       "      <td>0.204964</td>\n",
       "      <td>0.330645</td>\n",
       "      <td>0.675209</td>\n",
       "      <td>0.114403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305023</td>\n",
       "      <td>0.463645</td>\n",
       "      <td>0.621614</td>\n",
       "      <td>0.614746</td>\n",
       "      <td>0.401539</td>\n",
       "      <td>0.840944</td>\n",
       "      <td>0.074929</td>\n",
       "      <td>0.523992</td>\n",
       "      <td>0.074633</td>\n",
       "      <td>0.005774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>0.838305</td>\n",
       "      <td>0.187315</td>\n",
       "      <td>0.668331</td>\n",
       "      <td>0.625653</td>\n",
       "      <td>0.694821</td>\n",
       "      <td>0.305795</td>\n",
       "      <td>0.505483</td>\n",
       "      <td>0.220637</td>\n",
       "      <td>0.973320</td>\n",
       "      <td>0.472144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863375</td>\n",
       "      <td>0.903354</td>\n",
       "      <td>0.090747</td>\n",
       "      <td>0.823878</td>\n",
       "      <td>0.833744</td>\n",
       "      <td>0.254442</td>\n",
       "      <td>0.854606</td>\n",
       "      <td>0.282577</td>\n",
       "      <td>0.124343</td>\n",
       "      <td>0.716000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>0.998848</td>\n",
       "      <td>0.085800</td>\n",
       "      <td>0.471002</td>\n",
       "      <td>0.409039</td>\n",
       "      <td>0.700793</td>\n",
       "      <td>0.540194</td>\n",
       "      <td>0.107416</td>\n",
       "      <td>0.102830</td>\n",
       "      <td>0.798725</td>\n",
       "      <td>0.773601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026062</td>\n",
       "      <td>0.044855</td>\n",
       "      <td>0.712999</td>\n",
       "      <td>0.918720</td>\n",
       "      <td>0.893730</td>\n",
       "      <td>0.286155</td>\n",
       "      <td>0.116346</td>\n",
       "      <td>0.535174</td>\n",
       "      <td>0.483603</td>\n",
       "      <td>0.914082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6336</th>\n",
       "      <td>0.662911</td>\n",
       "      <td>0.213004</td>\n",
       "      <td>0.723441</td>\n",
       "      <td>0.183560</td>\n",
       "      <td>0.722360</td>\n",
       "      <td>0.145670</td>\n",
       "      <td>0.330431</td>\n",
       "      <td>0.135678</td>\n",
       "      <td>0.996597</td>\n",
       "      <td>0.220519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144963</td>\n",
       "      <td>0.446426</td>\n",
       "      <td>0.148142</td>\n",
       "      <td>0.398983</td>\n",
       "      <td>0.215387</td>\n",
       "      <td>0.081686</td>\n",
       "      <td>0.622973</td>\n",
       "      <td>0.427959</td>\n",
       "      <td>0.442991</td>\n",
       "      <td>0.751608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>0.416949</td>\n",
       "      <td>0.271127</td>\n",
       "      <td>0.791953</td>\n",
       "      <td>0.508140</td>\n",
       "      <td>0.039867</td>\n",
       "      <td>0.257467</td>\n",
       "      <td>0.701436</td>\n",
       "      <td>0.249736</td>\n",
       "      <td>0.430972</td>\n",
       "      <td>0.918784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435077</td>\n",
       "      <td>0.629779</td>\n",
       "      <td>0.088950</td>\n",
       "      <td>0.311274</td>\n",
       "      <td>0.453839</td>\n",
       "      <td>0.558268</td>\n",
       "      <td>0.585202</td>\n",
       "      <td>0.938641</td>\n",
       "      <td>0.548909</td>\n",
       "      <td>0.466086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.127408</td>\n",
       "      <td>0.799678</td>\n",
       "      <td>0.626792</td>\n",
       "      <td>0.124441</td>\n",
       "      <td>0.137988</td>\n",
       "      <td>0.041683</td>\n",
       "      <td>0.283041</td>\n",
       "      <td>0.592727</td>\n",
       "      <td>0.516547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854014</td>\n",
       "      <td>0.113504</td>\n",
       "      <td>0.449887</td>\n",
       "      <td>0.768145</td>\n",
       "      <td>0.033564</td>\n",
       "      <td>0.712753</td>\n",
       "      <td>0.263060</td>\n",
       "      <td>0.514465</td>\n",
       "      <td>0.901452</td>\n",
       "      <td>0.073480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>0.795114</td>\n",
       "      <td>0.455362</td>\n",
       "      <td>0.362424</td>\n",
       "      <td>0.136987</td>\n",
       "      <td>0.434703</td>\n",
       "      <td>0.682724</td>\n",
       "      <td>0.324495</td>\n",
       "      <td>0.025517</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.915626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546081</td>\n",
       "      <td>0.614907</td>\n",
       "      <td>0.510145</td>\n",
       "      <td>0.875002</td>\n",
       "      <td>0.102243</td>\n",
       "      <td>0.572923</td>\n",
       "      <td>0.600667</td>\n",
       "      <td>0.118417</td>\n",
       "      <td>0.279194</td>\n",
       "      <td>0.612196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 100000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6      \\\n",
       "5777  0.013107  0.470582  0.782615  0.356561  0.713228  0.001639  0.531202   \n",
       "3783  0.960434  0.443701  0.571535  0.813105  0.448731  0.292695  0.378250   \n",
       "1295  0.145563  0.462142  0.170783  0.475111  0.200657  0.647372  0.260204   \n",
       "838   0.415516  0.027677  0.297519  0.258756  0.415945  0.530397  0.204964   \n",
       "1786  0.838305  0.187315  0.668331  0.625653  0.694821  0.305795  0.505483   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3408  0.998848  0.085800  0.471002  0.409039  0.700793  0.540194  0.107416   \n",
       "6336  0.662911  0.213004  0.723441  0.183560  0.722360  0.145670  0.330431   \n",
       "1880  0.416949  0.271127  0.791953  0.508140  0.039867  0.257467  0.701436   \n",
       "763   0.647500  0.127408  0.799678  0.626792  0.124441  0.137988  0.041683   \n",
       "2631  0.795114  0.455362  0.362424  0.136987  0.434703  0.682724  0.324495   \n",
       "\n",
       "         7         8         9      ...     99990     99991     99992  \\\n",
       "5777  0.149542  0.238533  0.156730  ...  0.014578  0.703602  0.121921   \n",
       "3783  0.988330  0.737386  0.240126  ...  0.865072  0.048426  0.556778   \n",
       "1295  0.386845  0.420123  0.275254  ...  0.038677  0.744098  0.731141   \n",
       "838   0.330645  0.675209  0.114403  ...  0.305023  0.463645  0.621614   \n",
       "1786  0.220637  0.973320  0.472144  ...  0.863375  0.903354  0.090747   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3408  0.102830  0.798725  0.773601  ...  0.026062  0.044855  0.712999   \n",
       "6336  0.135678  0.996597  0.220519  ...  0.144963  0.446426  0.148142   \n",
       "1880  0.249736  0.430972  0.918784  ...  0.435077  0.629779  0.088950   \n",
       "763   0.283041  0.592727  0.516547  ...  0.854014  0.113504  0.449887   \n",
       "2631  0.025517  0.901961  0.915626  ...  0.546081  0.614907  0.510145   \n",
       "\n",
       "         99993     99994     99995     99996     99997     99998     99999  \n",
       "5777  0.809777  0.141529  0.051837  0.615442  0.598923  0.463951  0.278275  \n",
       "3783  0.939422  0.430007  0.304750  0.629446  0.784474  0.027791  0.411822  \n",
       "1295  0.329909  0.280043  0.441809  0.997663  0.531609  0.461333  0.930565  \n",
       "838   0.614746  0.401539  0.840944  0.074929  0.523992  0.074633  0.005774  \n",
       "1786  0.823878  0.833744  0.254442  0.854606  0.282577  0.124343  0.716000  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3408  0.918720  0.893730  0.286155  0.116346  0.535174  0.483603  0.914082  \n",
       "6336  0.398983  0.215387  0.081686  0.622973  0.427959  0.442991  0.751608  \n",
       "1880  0.311274  0.453839  0.558268  0.585202  0.938641  0.548909  0.466086  \n",
       "763   0.768145  0.033564  0.712753  0.263060  0.514465  0.901452  0.073480  \n",
       "2631  0.875002  0.102243  0.572923  0.600667  0.118417  0.279194  0.612196  \n",
       "\n",
       "[10000 rows x 100000 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of columns\n",
    "w = 100000\n",
    "\n",
    "# Number of rows for df\n",
    "x = 10000 \n",
    "\n",
    "# Number of rows for df2\n",
    "y = 1000\n",
    "\n",
    "# Number of rows for df2\n",
    "z = 100\n",
    "\n",
    "# We fix the seed so that random number do not change everytime the script is launched\n",
    "np.random.seed(0)\n",
    "\n",
    "# Creating a dataframe with x number of rows and y number of columns\n",
    "df = pd.DataFrame(np.random.random_sample((x,w)))\n",
    "\n",
    "# Shuffle rows of dataframe\n",
    "df = df.sample(frac=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{table}\\n\\\\centering\\n\\\\caption{Dataframe}\\n\\\\label{Datraframe}\\n\\\\begin{tabular}{rrrr}\\n\\\\toprule\\n    1     &     2     &     99998 &     99999 \\\\\\\\\\n\\\\midrule\\n 0.470582 &  0.782615 &  0.463951 &  0.278275 \\\\\\\\\\n 0.443701 &  0.571535 &  0.027791 &  0.411822 \\\\\\\\\\n 0.462142 &  0.170783 &  0.461333 &  0.930565 \\\\\\\\\\n 0.027677 &  0.297519 &  0.074633 &  0.005774 \\\\\\\\\\n 0.187315 &  0.668331 &  0.124343 &  0.716000 \\\\\\\\\\n\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass table header to latex\n",
    "df.head().to_latex(index=False,columns=[1,2,99998,99999], caption=\"Dataframe\", label=\"Datraframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess the correlations of the different parameters, correlations between the column with index 0 and the remaining 99998 other columns is assessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 99999) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Assigning X to all columns except 0\n",
    "X_df = df.drop(columns=0)\n",
    "X_df.head()\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df = df[0]\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df = np.array(Y_df).reshape(-1)\n",
    "print(X_df.shape,Y_df.shape)\n",
    "\n",
    "# Calculate correlations between selected column and column 0\n",
    "list_titles = X_df.columns\n",
    "list_corr_df = []\n",
    "for i in list_titles:\n",
    "    list_corr_df.append(abs(np.corrcoef(Y_df, X_df[i])[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how correlations between the different parameters changes due to the data size, the original dataframe is compared to sub-set dataframes which only takes the first y or z rows of the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 99999) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Creating smaller dataframe taking y number of rows from original dataframe\n",
    "df2 = df.iloc[:y]\n",
    "\n",
    "# Assigning X to all columns except 0\n",
    "X_df2 = df2.drop(columns=0)\n",
    "X_df2.head()\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df2 = df2[0]\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df2 = np.array(Y_df2).reshape(-1)\n",
    "print(X_df2.shape,Y_df2.shape)\n",
    "\n",
    "list_titles = X_df2.columns\n",
    "\n",
    "# Calculate correlations between selected column and column 0\n",
    "list_corr_df2 = []\n",
    "for i in list_titles:\n",
    "    list_corr_df2.append(abs(np.corrcoef(Y_df2, X_df2[i])[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 99999) (100,)\n"
     ]
    }
   ],
   "source": [
    "# Creating smaller dataframe taking z number of rows from original dataframe\n",
    "df3 = df.iloc[:z]\n",
    "\n",
    "# Assigning X to all columns except 0\n",
    "X_df3 = df3.drop(columns=0)\n",
    "X_df3.head()\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df3 = df3[0]\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df3 = np.array(Y_df3).reshape(-1)\n",
    "print(X_df3.shape,Y_df3.shape)\n",
    "\n",
    "list_titles = X_df3.columns\n",
    "\n",
    "# Calculate correlations between selected column and column 0\n",
    "list_corr_df3 = []\n",
    "for i in list_titles:\n",
    "    list_corr_df3.append(abs(np.corrcoef(Y_df3, X_df3[i])[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare which of the dataframes has the highest proportions of correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check number of columns df: 99999\n",
      "Check number of rows df: 10000\n",
      "Percentage of correlations in df: 31.997319973199733 %\n",
      "\n",
      "Check number of columns df2: 99999\n",
      "Check number of rows df2: 1000\n",
      "Percentage of correlations in df2: 75.16475164751647 %\n",
      "\n",
      "Check number of columns df3: 99999\n",
      "Check number of rows df3: 100\n",
      "Percentage of correlations in df3: 92.07692076920769 %\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "\n",
    "for i in range(0,len(list_corr_df)):\n",
    "    if abs(list_corr_df[i]) > 0.01:\n",
    "        a+=1\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "for i in range(0,len(list_corr_df2)):\n",
    "    if abs(list_corr_df2[i]) > 0.01:\n",
    "        b+=1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "for i in range(0,len(list_corr_df3)):\n",
    "    if abs(list_corr_df3[i]) > 0.01:\n",
    "        c+=1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "print(\"Check number of columns df:\", len(X_df.columns))\n",
    "print(\"Check number of rows df:\", len(X_df))\n",
    "print(\"Percentage of correlations in df:\",(a/len(list_corr_df)*100),\"%\")\n",
    "print()\n",
    "print(\"Check number of columns df2:\", len(X_df2.columns))\n",
    "print(\"Check number of rows df2:\", len(X_df2))\n",
    "print(\"Percentage of correlations in df2:\",(b/len(list_corr_df2)*100),\"%\")\n",
    "print()\n",
    "print(\"Check number of columns df3:\", len(X_df3.columns))\n",
    "print(\"Check number of rows df3:\", len(X_df3))\n",
    "print(\"Percentage of correlations in df3:\",(c/len(list_corr_df3)*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly it is seen that the lower the number of observations, the higher the frequency of spurious correlations. However the number of spurious correlations present in the bigger DataFrame is still very significant. This would mean that the dimensionality of the data also plays an important role for the precense of spurious correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To counter spurious correlations, Principal Component Analysis and Random Projection can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PCA, it is up to the user to choose how many dimension the final data has. This is because the explained variance tells you how much information (variance) can be attributed to each of the principal components. This is important as while you can convert a y dimensional space to 2 dimensional space, you lose some of the variance (information) when you do this. It is up to the user to decide how much variance he is ready to lose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00015985 0.00015956]\n"
     ]
    }
   ],
   "source": [
    "# Here we pass the daframe of y columns in 2 dimensions\n",
    "pca = PCA(n_components = 2)\n",
    "df_pca = pd.DataFrame(pca.fit_transform(df))\n",
    "\n",
    "# How much of the information is kept by the new dataframe?\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of ratios for 2 n_components: 0.00031941487308860604\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum of ratios for 2 n_components:\",pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataframe is not useful as it almost does not capture any variance (only 0.03%). Thus we need more dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00016387 0.00016353 0.00016323 0.00016313 0.00016284 0.00016265\n",
      " 0.00016251 0.00016239 0.00016217 0.00016212 0.00016195 0.00016172\n",
      " 0.0001616  0.00016143 0.00016135 0.00016132 0.00016116 0.00016094\n",
      " 0.0001608  0.0001607  0.00016066 0.00016045 0.00016043 0.00016028\n",
      " 0.00015622 0.00015614 0.00015594 0.00015585 0.0001558  0.00015568\n",
      " 0.00015555 0.00015549 0.00015546 0.00015528 0.00015521 0.00015516\n",
      " 0.00015499 0.00015491 0.00015481 0.00015472 0.00015463 0.00015448\n",
      " 0.00015439 0.00015431 0.0001542  0.00015402 0.00015397 0.0001538\n",
      " 0.00015372 0.00015359 0.00015348 0.00015339 0.00015333 0.00015323\n",
      " 0.00015312 0.00015288 0.00015285 0.0001527 ]\n"
     ]
    }
   ],
   "source": [
    "# Here we pass the daframe of y columns in 100 dimensions\n",
    "pca = PCA(n_components = 100)\n",
    "df_pca = pd.DataFrame(pca.fit_transform(df))\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of ratios for 100 n_components: 0.015780478235050775\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum of ratios for 100 n_components:\",pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataframe explains only 1.63% of the variance. However it seems that everytime we multiple by 10 the number of columns, the variance is multiplied by by 10 too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00016629 0.0001661  0.00016581 0.0001658  0.00016558 0.00016544\n",
      " 0.00016529 0.00016525 0.00016514 0.00016512 0.00016487 0.00016484\n",
      " 0.00016475 0.00016462 0.00016456 0.00016449 0.0001644  0.00016422\n",
      " 0.00016414 0.00016412 0.00016406 0.00016396 0.00016389 0.00016374\n",
      " 0.00012565 0.00012552 0.00012546 0.00012543]\n"
     ]
    }
   ],
   "source": [
    "# Here we pass the daframe of y columns in 1000 dimensions\n",
    "pca = PCA(n_components = 1000)\n",
    "df_pca = pd.DataFrame(pca.fit_transform(df))\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of ratios for 1000 n_components: 0.14460176836621738\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum of ratios for 1000 n_components:\",pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 1000 Columns, the sum of all the variances is close to 10%, therefore appplying PCA on data with dimension 100000 and changing it to 10000 dimensions should effective as we see that the variance is multipled by 10 everytime the number of n_components is multiplied by 10 which would result with a variance close to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.73222709e-04 1.73103907e-04 1.72949852e-04 ... 4.68212754e-05\n",
      " 4.67843489e-05 2.20717127e-29]\n"
     ]
    }
   ],
   "source": [
    "# Here we pass the daframe of y columns in 1000 dimensions\n",
    "pca = PCA(n_components = 10000)\n",
    "df_pca = pd.DataFrame(pca.fit_transform(df))\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of ratios for 10000 n_components: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum of ratios for 10000 n_components:\",pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 10000 Columns, the sum of all the variances is almost equal to 100% explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9999) (10000,)\n"
     ]
    }
   ],
   "source": [
    "df_new = df_pca\n",
    "# Assigning X to all columns except 0\n",
    "X_df_new = df_new.drop(columns=0)\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df_new = df_new[0]\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df_new = np.array(Y_df_new).reshape(-1)\n",
    "print(X_df_new.shape,Y_df_new.shape)\n",
    "\n",
    "list_titles = X_df_new.columns\n",
    "\n",
    "# Calculate correlations between selected column and column 0\n",
    "list_corr_df_new = []\n",
    "for i in list_titles[0:len(list_titles)]:\n",
    "    list_corr_df_new.append(abs(np.corrcoef(Y_df_new, X_df_new[i])[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correlations in df1: 31.997319973199733 %\n",
      "Percentage of correlations in df_new: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "for i in range(0,len(list_corr_df)):\n",
    "    if abs(list_corr_df[i]) > 0.01:\n",
    "        a+=1\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "for i in range(0,len(list_corr_df_new)):\n",
    "    if abs(list_corr_df_new[i]) > 0.01:\n",
    "        b+=1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"Percentage of correlations in df1:\",(a/len(list_corr_df)*100),\"%\")\n",
    "\n",
    "print(\"Percentage of correlations in df_new:\",(b/len(list_corr_df_new)*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation of the original DataFrame is close to 0 when transformed through PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA seems to be able to counter spurious correlations by reducing from dimension 100000 to 10000. However it is extremely computationally expensive therefore a faster method could be used, such a Random Projection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Random Projection, the parameter eps defines how much the Random Projection can deviate from original DataFrame.\n",
    "The lower the percentage of the parameter the higher the fidelity of the transformed data.\n",
    "However the higher the fidelity the less the reduction of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a reduced data with a distortion of 10% for a dataset of 10000 rows will have 7894 columns to present the same characteristics as the orginal dataframe as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7894"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "johnson_lindenstrauss_min_dim(n_samples=10000, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 30488) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Pass df in the random projection to create a new reduced DataFrame with eps = 0.05\n",
    "transformer = random_projection.GaussianRandomProjection(eps = 0.05)\n",
    "df_new = pd.DataFrame(transformer.fit_transform(df))\n",
    "\n",
    "# Assigning X to all columns except 0\n",
    "X_df_new = df_new.drop(columns=0)\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df_new = df_new[0]\n",
    "\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df_new = np.array(Y_df_new).reshape(-1)\n",
    "print(X_df_new.shape,Y_df_new.shape)\n",
    "\n",
    "list_titles = X_df_new.columns\n",
    "\n",
    "list_corr_df_zerozero5 = []\n",
    "for i in list_titles[0:len(list_titles)]:\n",
    "    list_corr_df_zerozero5.append(abs(np.corrcoef(Y_df_new, X_df_new[i])[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 7893) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Pass df in the random projection to create a new reduced DataFrame with eps = 0.1\n",
    "transformer = random_projection.GaussianRandomProjection(eps = 0.1)\n",
    "df_new = pd.DataFrame(transformer.fit_transform(df))\n",
    "\n",
    "# Assigning X to all columns except 0\n",
    "X_df_new = df_new.drop(columns=0)\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df_new = df_new[0]\n",
    "\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df_new = np.array(Y_df_new).reshape(-1)\n",
    "print(X_df_new.shape,Y_df_new.shape)\n",
    "\n",
    "list_titles = X_df_new.columns\n",
    "\n",
    "list_corr_df_zero1 = []\n",
    "for i in list_titles[0:len(list_titles)]:\n",
    "    list_corr_df_zero1.append(abs(np.corrcoef(Y_df_new, X_df_new[i])[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2124) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Pass df in the random projection to create a new reduced DataFrame with eps = 0.2\n",
    "transformer = random_projection.GaussianRandomProjection(eps = 0.2)\n",
    "df_new = pd.DataFrame(transformer.fit_transform(df))\n",
    "\n",
    "# Assigning X to all columns except 0\n",
    "X_df_new = df_new.drop(columns=0)\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df_new = df_new[0]\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df_new = np.array(Y_df_new).reshape(-1)\n",
    "print(X_df_new.shape,Y_df_new.shape)\n",
    "\n",
    "list_titles = X_df_new.columns\n",
    "\n",
    "list_corr_df_zero2 = []\n",
    "for i in list_titles[0:len(list_titles)]:\n",
    "    list_corr_df_zero2.append(abs(np.corrcoef(Y_df_new, X_df_new[i])[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 441) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Pass df in the random projection to create a new reduced DataFrame with eps = 0.5\n",
    "transformer = random_projection.GaussianRandomProjection(eps = 0.5)\n",
    "df_new = pd.DataFrame(transformer.fit_transform(df))\n",
    "\n",
    "# Assigning X to all columns except 0\n",
    "X_df_new = df_new.drop(columns=0)\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df_new = df_new[0]\n",
    "\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df_new = np.array(Y_df_new).reshape(-1)\n",
    "print(X_df_new.shape,Y_df_new.shape)\n",
    "\n",
    "list_titles = X_df_new.columns\n",
    "\n",
    "list_corr_df_zero5 = []\n",
    "for i in list_titles[0:len(list_titles)]:\n",
    "    list_corr_df_zero5.append(abs(np.corrcoef(Y_df_new, X_df_new[i])[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 245) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Pass df in the random projection to create a new reduced DataFrame with eps = 0.8\n",
    "transformer = random_projection.GaussianRandomProjection(eps = 0.8)\n",
    "df_new = pd.DataFrame(transformer.fit_transform(df))\n",
    "\n",
    "# Assigning X to all columns except 0\n",
    "X_df_new = df_new.drop(columns=0)\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df_new = df_new[0]\n",
    "\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df_new = np.array(Y_df_new).reshape(-1)\n",
    "print(X_df_new.shape,Y_df_new.shape)\n",
    "\n",
    "list_titles = X_df_new.columns\n",
    "\n",
    "list_corr_df_zero8 = []\n",
    "for i in list_titles[0:len(list_titles)]:\n",
    "    list_corr_df_zero8.append(abs(np.corrcoef(Y_df_new, X_df_new[i])[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correlations in df: 31.997319973199733 %\n",
      "Percentage of correlations in df_zerozero5: 34.052742062450804 %\n",
      "Percentage of correlations in df_zero1: 33.764094767515516 %\n",
      "Percentage of correlations in df_zero2: 33.75706214689266 %\n",
      "Percentage of correlations in df_zero5: 32.426303854875286 %\n",
      "Percentage of correlations in df_zero8: 30.20408163265306 %\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "d = 0\n",
    "e = 0\n",
    "f = 0\n",
    "\n",
    "for i in range(0,len(list_corr_df)):\n",
    "    if abs(list_corr_df[i]) > 0.01:\n",
    "        a+=1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "for i in range(0,len(list_corr_df_zerozero5)):\n",
    "    if abs(list_corr_df_zerozero5[i]) > 0.01:\n",
    "        b+=1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "for i in range(0,len(list_corr_df_zero1)):\n",
    "    if abs(list_corr_df_zero1[i]) > 0.01:\n",
    "        c+=1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for i in range(0,len(list_corr_df_zero2)):\n",
    "    if abs(list_corr_df_zero2[i]) > 0.01:\n",
    "        d+=1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for i in range(0,len(list_corr_df_zero5)):\n",
    "    if abs(list_corr_df_zero5[i]) > 0.01:\n",
    "        e+=1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for i in range(0,len(list_corr_df_zero8)):\n",
    "    if abs(list_corr_df_zero8[i]) > 0.01:\n",
    "        f+=1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"Percentage of correlations in df:\",(a/len(list_corr_df)*100),\"%\")\n",
    "print(\"Percentage of correlations in df_zerozero5:\",(b/len(list_corr_df_zerozero5)*100),\"%\")\n",
    "print(\"Percentage of correlations in df_zero1:\",(c/len(list_corr_df_zero1)*100),\"%\")\n",
    "print(\"Percentage of correlations in df_zero2:\",(d/len(list_corr_df_zero2)*100),\"%\")\n",
    "print(\"Percentage of correlations in df_zero5:\",(e/len(list_corr_df_zero5)*100),\"%\")\n",
    "print(\"Percentage of correlations in df_zero8:\",(f/len(list_corr_df_zero8)*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Projection seems to actually increase the number of spurious correlations. Changing the precision of the embeddings through the \"eps\" parameter influences slightly the correlations but not to a significant level. Random Projection seems to be limited as it is found to be enable to lead to valid results for highly dimensional random Big Data. Random Projection is nonetheless useful in reducing the amount of columns in the original dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spurious correlations can be avoided in Big Data thanks to Dimension Reduction methods.\n",
    "\n",
    "PCA is very precise but extremely computationally expensive due to high dimensionality.\n",
    "\n",
    "Random Projection is very quick but lacks precision due to trade off between distortion and correlation.\n",
    "\n",
    "Future research could find another algorithm which is suitable for high dimensional data which is quick, reliable and does not distort the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
