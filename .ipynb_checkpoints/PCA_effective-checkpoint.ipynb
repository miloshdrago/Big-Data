{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spurious Correlations of Highly Dimensional Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook aims at showing how PCA and random projection can solve the problem of spurious correlations in Big Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import random_projection\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined parameters\n",
    "\n",
    "# Number of rows for df1\n",
    "x = 10000\n",
    "\n",
    "# Number of columns for df1\n",
    "y = 100000\n",
    "\n",
    "# Number of rows for df2\n",
    "z = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with x number of rows and y number of columns\n",
    "df = pd.DataFrame(np.random.random_sample((x,y)))\n",
    "\n",
    "# Shuffle rows of dataframe\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess the correlations of the different parameters, correlations between the column with index 0 and the 999 other first columns is assessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assigning X to all columns except 0\n",
    "X_df = df.drop(columns=0)\n",
    "X_df.head()\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df = df[0]\n",
    "print(Y_df)\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df = np.array(Y_df).reshape(-1)\n",
    "print(X_df.shape,Y_df.shape)\n",
    "\n",
    "# Calculate correlations between selected column and column 0\n",
    "list_titles = X_df.columns\n",
    "list_corr_df1 = []\n",
    "for i in list_titles[0:z]:\n",
    "    list_corr_df1.append(abs(np.corrcoef(Y_df, X_df[i])[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if indeed the correlations between the different parameters increases with the data size, the original dataframe is compared to a sub-set dataframe which only takes the first z rows of the original dataframe. If the correlations in the original dataframe are higher than in the smaller dataframe this would prove that the bigger the data size the more frequent the number of spurious correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating smaller dataframe taking z number of rows from original dataframe\n",
    "df2 = df.iloc[:z]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assigning X to all columns except 0\n",
    "X_df2 = df2.drop(columns=0)\n",
    "X_df2.head()\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df2 = df2[0]\n",
    "print(Y_df2)\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df2 = np.array(Y_df2).reshape(-1)\n",
    "print(X_df2.shape,Y_df2.shape)\n",
    "\n",
    "list_titles = X_df2.columns\n",
    "\n",
    "# Calculate correlations between selected column and column 0\n",
    "list_corr_df2 = []\n",
    "for i in list_titles[0:z]:\n",
    "    list_corr_df2.append(abs(np.corrcoef(Y_df2, X_df2[i])[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare which of the correlation lists has the highest numbers of every parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "for i in range(0,len(list_corr_df1)):\n",
    "    if abs(list_corr_df1[i]) > 0.01:\n",
    "        a+=1\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "for i in range(0,len(list_corr_df2)):\n",
    "    if abs(list_corr_df2[i]) > 0.01:\n",
    "        b+=1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"Percentage of correlations in df1:\",(a/len(list_corr_df1)*100),\"%\")\n",
    "\n",
    "print(\"Percentage of correlations in df2:\",(b/len(list_corr_df2)*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly it is seen that the lower the number of observations, the higher the frequency of spurious correlations. However the number of spurious correlations present in the bigger DataFrame is still very significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To counter spurious correlations, Principal Component Analysis and Random Projection can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PCA, it is up to the user to choose how many dimension the final data has. This is because the explained variance tells you how much information (variance) can be attributed to each of the principal components. This is important as while you can convert a y dimensional space to 2 dimensional space, you lose some of the variance (information) when you do this. It is up to the user to decide how much variance he is ready to lose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we pass the daframe of y columns in 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pd.DataFrame(pca.fit_transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much of the information is kept by the new dataframe?\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataframe is not useful as it almost does not capture any variance. Thus we need more dimensionss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we pass the daframe of y columns in 2 dimensions\n",
    "pca = PCA(n_components=100)\n",
    "df_pca = pd.DataFrame(pca.fit_transform(df))\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we pass the daframe of y columns in 2 dimensions\n",
    "pca = PCA(n_components=1000)\n",
    "df_pca = pd.DataFrame(pca.fit_transform(df))\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_pca\n",
    "# Assigning X to all columns except 0\n",
    "X_df_new = df_new.drop(columns=0)\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df_new = df_new[0]\n",
    "print(Y_df_new)\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df_new = np.array(Y_df_new).reshape(-1)\n",
    "print(X_df_new.shape,Y_df_new.shape)\n",
    "\n",
    "list_titles = X_df_new.columns\n",
    "\n",
    "# Calculate correlations between selected column and column 0\n",
    "list_corr_df_new = []\n",
    "for i in list_titles[0:len(list_titles)]:\n",
    "    list_corr_df_new.append(abs(np.corrcoef(Y_df_new, X_df_new[i])[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "for i in range(0,len(list_corr_df1)):\n",
    "    if abs(list_corr_df1[i]) > 0.01:\n",
    "        a+=1\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "for i in range(0,len(list_corr_df_new)):\n",
    "    if abs(list_corr_df_new[i]) > 0.01:\n",
    "        b+=1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"Percentage of correlations in df1:\",(a/len(list_corr_df1)*100),\"%\")\n",
    "\n",
    "print(\"Percentage of correlations in df_new:\",(b/len(list_corr_df_new)*100),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Random Projection, the parameter eps defines how much the Random Projection can deviate from original DataFrame.\n",
    "The lower the percentage of the parameter the higher the fidelity of the transformed data.\n",
    "However the higher the fidelity the less the reduction of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = random_projection.GaussianRandomProjection(eps = 0.1)\n",
    "df_rp = pd.DataFrame(transformer.fit_transform(df))\n",
    "df_rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_rp\n",
    "# Assigning X to all columns except 0\n",
    "X_df_new = df_new.drop(columns=0)\n",
    "\n",
    "# Assigning Y to column 0\n",
    "Y_df_new = df_new[0]\n",
    "print(Y_df_new)\n",
    "\n",
    "# The following line makes Y become a list\n",
    "Y_df_new = np.array(Y_df_new).reshape(-1)\n",
    "print(X_df_new.shape,Y_df_new.shape)\n",
    "\n",
    "list_titles = X_df_new.columns\n",
    "\n",
    "list_corr_df_new = []\n",
    "for i in list_titles[0:len(list_titles)]:\n",
    "    print(i)\n",
    "    list_corr_df_new.append(abs(np.corrcoef(Y_df_new, X_df_new[i])[0][1]))\n",
    "    print('Correlation matrix for column 0 and and column' + str(i) + ': ' + str(np.corrcoef(Y_df_new, X_df_new[i])[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "for i in range(0,len(list_corr_df1)):\n",
    "    if abs(list_corr_df1[i]) > 0.01:\n",
    "        a+=1\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "for i in range(0,len(list_corr_df_new)):\n",
    "    if abs(list_corr_df_new[i]) > 0.01:\n",
    "        b+=1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"Percentage of correlations in df1:\",(a/len(list_corr_df1)*100),\"%\")\n",
    "\n",
    "print(\"Percentage of correlations in df_new:\",(b/len(list_corr_df_new)*100),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
